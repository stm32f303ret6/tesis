@startuml Relacion_Datos_Aprendizaje
!theme plain
skinparam classAttributeIconSize 0
skinparam roundcorner 10
skinparam linetype ortho
skinparam shadowing false

title Conceptos de RL vs Implementación (Vista Resumida)

package "Conceptos de RL" #F4F4F4 {
    abstract class "Estado (s)" as State
    abstract class "Acción (a)" as Action
    abstract class "Recompensa (r)" as Reward
    abstract class "Política π" as Policy
    abstract class "Valor V(s)" as ValueFunc
    abstract class "Ventaja A(s,a)" as Advantage
    abstract class "Trayectoria τ" as Trajectory
}

package "Implementación en Código" #E7F0FF {
    class ObservationVector {
        + dim: 65
    }
    class ActionVector {
        + dim: 12
    }
    class RewardScalar {
        + float32
    }
    class ActorCriticPolicy {
        + actor/critic MLP
    }
    class CriticNetwork {
        + salida: V(s)
    }
    class AdvantageArray {
        + shape: (4096, 80, 1)
    }
    class RolloutBuffer {
        + memory: 327680 transiciones
    }
    class NormalizationStats {
        + RunningMeanStd
    }
}

State ..|> ObservationVector : implementa
Action ..|> ActionVector
Reward ..|> RewardScalar
Policy ..|> ActorCriticPolicy
ValueFunc ..|> CriticNetwork
Advantage ..|> AdvantageArray
Trajectory ..|> RolloutBuffer

ObservationVector --> NormalizationStats : normaliza
NormalizationStats --> ActorCriticPolicy : entrada
ActorCriticPolicy --> ActionVector : produce
ActionVector --> RewardScalar : evalúa
RewardScalar --> RolloutBuffer : guarda
RolloutBuffer --> AdvantageArray : calcula
AdvantageArray --> ActorCriticPolicy : actualiza pesos

@enduml
