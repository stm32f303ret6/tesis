@startuml Flujo_de_Entrenamiento
!theme plain
skinparam sequenceMessageAlign center
skinparam roundcorner 10
skinparam shadowing false

title Flujo Simplificado del Entrenamiento Residual PPO

actor Usuario
participant "TrainingScript" as Script
participant "PPOAgent" as PPO
participant "VecEnv\n(80 envs)" as VecEnv
participant "ResidualWalkEnv" as Env
participant "BezierController" as Controller
participant "MuJoCo" as MuJoCo
participant "RolloutBuffer" as Buffer

Usuario -> Script: python train_residual_ppo_v3.py
Script -> VecEnv: crear entornos vectorizados
VecEnv -> Env: instanciar ResidualWalkEnv
Script -> PPO: inicializar (policy, config)
PPO -> Buffer: preparar memoria (n_steps, n_envs)

group Recolección (n_steps = 4096)
    PPO -> VecEnv: solicitar observaciones normalizadas
    VecEnv -> Env: step(action_t)
    Env -> Controller: update(residuals_t)
    Controller --> Env: objetivos pata
    Env -> MuJoCo: apply_control(joint_angles)
    MuJoCo --> Env: nuevo estado s_{t+1}
    Env --> VecEnv: obs', reward, done
    VecEnv --> PPO: lote vectorizado
    PPO -> Buffer: add(obs, action, reward, value, log_prob, done)
end

group Cálculo de ventajas (GAE)
    PPO -> Buffer: compute_advantages(gamma, gae_lambda)
end

group Optimización (n_epochs = 10)
    PPO -> Buffer: sample mini-batches (batch_size = 2048)
    PPO -> PPO: actualizar policy/value\n(backprop + clip)
end

group Logging y checkpoints
    PPO --> Script: métricas (rew, len, losses)
    Script -> Script: guardar modelo y vec_norm
end

Script --> Usuario: progreso / entrenamiento completo

@enduml
