# Documentación de Diagramas UML

Esta carpeta contiene los diagramas UML que modelan la arquitectura del sistema de aprendizaje por refuerzo para control de marcha de un robot cuadrúpedo con patas SCARA paralelas 3DOF.

## Descripción de los Diagramas

### 01_estructuras_datos.puml - Estructuras de Datos Principales

El primer diagrama modela las estructuras de datos fundamentales utilizadas en el sistema de aprendizaje por refuerzo residual con PPO, organizadas en tres paquetes conceptuales. El paquete de observación y acción define el espacio de estados que el agente percibe, incluyendo información del cuerpo, articulaciones, posiciones de los pies y contactos con el suelo, así como el espacio de acciones que representa los residuales aplicados al controlador base de marcha. El paquete de parámetros y control estructura tanto la configuración del generador de marcha diagonal (altura del cuerpo, longitud y altura del paso, tiempo de ciclo) como los hiperparámetros del proceso de entrenamiento (número de pasos, tamaño de lote, tasa de aprendizaje, escala de residuales). Finalmente, el paquete de almacenamiento y métricas representa los buffers de memoria utilizados para almacenar las trayectorias recolectadas durante el entrenamiento y las métricas que permiten monitorear el progreso del aprendizaje. Este diagrama permite comprender rápidamente qué información observa el agente, cómo actúa sobre el sistema, y qué datos se almacenan durante el proceso de aprendizaje.

### 02_sistema_completo.puml - Arquitectura del Sistema Residual PPO

El segundo diagrama presenta una vista arquitectural completa del sistema organizado en cuatro capas funcionales que interactúan entre sí. La capa de simulación encapsula la interacción con el motor de físicas MuJoCo, responsable de simular la dinámica del robot y el terreno rugoso. La capa de control contiene el generador de marcha diagonal que produce trayectorias Bézier para los pies, el controlador residual que modifica estas trayectorias con las correcciones aprendidas, y los solucionadores de cinemática inversa que convierten posiciones objetivo de los pies en ángulos articulares. La capa de aprendizaje integra el entorno compatible con Gym que conecta el control con el agente PPO, incluyendo la red neuronal actor-crítico que implementa la política y el buffer de experiencias que almacena las transiciones para el entrenamiento. Por último, la capa de gestión proporciona componentes auxiliares como la vectorización de entornos para paralelización, normalización de observaciones y acciones, callbacks para guardar checkpoints, y sistemas de logging para monitoreo del entrenamiento. Este diagrama permite visualizar cómo se integran el controlador base, el aprendizaje por refuerzo y la simulación física en un sistema cohesivo.

### 03_flujo_entrenamiento.puml - Flujo de Entrenamiento

El tercer diagrama es un diagrama de secuencia que ilustra el flujo temporal completo del proceso de entrenamiento con PPO, desde la inicialización hasta la evaluación final. El proceso comienza con la creación de múltiples entornos vectorizados en paralelo y la inicialización del agente PPO con su política y configuración de hiperparámetros. Durante la fase de recolección, el agente interactúa con los entornos durante un número determinado de pasos, donde en cada iteración el agente observa el estado actual, ejecuta una acción basada en su política, recibe una recompensa del entorno, y almacena toda esta información en el buffer de experiencias. Una vez recolectadas suficientes experiencias, se aplica el algoritmo GAE (Generalized Advantage Estimation) para calcular las ventajas de cada par estado-acción, lo que permite estimar qué tan buena fue cada decisión en comparación con el comportamiento esperado. Posteriormente, durante la fase de optimización, se muestrean mini-lotes del buffer y se actualizan los pesos de las redes actor y crítico mediante backpropagation, utilizando el mecanismo de clipping característico de PPO para mantener actualizaciones conservadoras. Finalmente, se registran las métricas de rendimiento y se guardan checkpoints del modelo y las estadísticas de normalización. Este diagrama proporciona una visión clara del pipeline de entrenamiento, útil para explicar la metodología de aprendizaje en la tesis.

### 04_relacion_datos_aprendizaje.puml - Conceptos RL vs Implementación

El cuarto diagrama establece un puente entre la teoría abstracta de aprendizaje por refuerzo y su implementación concreta en este sistema, permitiendo conectar el marco teórico con la arquitectura real. En el lado izquierdo se presentan los conceptos fundamentales de RL como entidades abstractas: el estado que representa la situación del agente, la acción que puede tomar, la recompensa que recibe, la política que mapea estados a acciones, la función de valor que estima retornos futuros, la ventaja que mide qué tan buena es una acción respecto al promedio, y la trayectoria que es una secuencia de transiciones. En el lado derecho se muestra cómo cada concepto teórico se materializa en componentes concretos del sistema: el estado se implementa como un vector de observación de 65 dimensiones, la acción como un vector de 12 dimensiones, la recompensa como un escalar en punto flotante, la política como una red neuronal actor-crítico con múltiples capas, la función de valor como la red crítica que produce estimaciones V(s), la ventaja como un arreglo tridimensional que almacena las ventajas calculadas para todas las experiencias recolectadas, y la trayectoria como un buffer de memoria que contiene miles de transiciones. Las relaciones entre estos componentes muestran cómo los datos fluyen desde la observación inicial, pasan por la normalización estadística, son procesados por la política para generar acciones, estas acciones son evaluadas por el entorno para producir recompensas, las cuales se almacenan en el buffer junto con las observaciones, y finalmente se utilizan para calcular ventajas que actualizan los pesos de la política. Este diagrama ayuda a establecer la conexión entre el marco teórico de la tesis y la arquitectura implementada, facilitando la comprensión para lectores con conocimiento de RL pero sin experiencia en esta implementación específica.

---

Los diagramas utilizan PlantUML como formato para facilitar el versionado y la edición. Las dimensiones y parámetros específicos corresponden a la configuración actual del sistema, pero pueden variar según cambios en el diseño. Para visualizar los diagramas, se puede usar el script renderizar_uml.py o cualquier herramienta compatible con PlantUML. Este conjunto de diagramas fue desarrollado en noviembre de 2025 como parte del sistema de control de marcha para robot cuadrúpedo con aprendizaje por refuerzo residual basado en PPO.
